---
title: "Inferential Data Analysis"
author: "Ian Dinwoodie"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(car)
knitr::opts_chunk$set(echo=TRUE)
```

# Loading the Data

Load the raw data and verify its dimensions and structure.

```{r}
df <- readRDS('../data/tidy.Rds')
dim(df)
summary(df)
```

# Control vs Experimental Group

## Overview

The first question we wanted to answer was if training at a young age (i.e.,
puppy training) would have an impact on the likelihood of a dog having certain
behavior problems. The behavior problems we are exploring are:

- Aggression
- Barking (excessively)
- Compulsion
- Coprophagia
- Destructive behavior
- Escaping/running away
- Fear/anxiety
- House soiling
- Hyperactivity/overactivity
- Mounting
- Problematic jumping
- Rolling in repulsive materials

We initialize a vector to hold these outcomes.

```{r}
outcomes <- c(
  'aggression',
  'fear_anxiety',
  'jumping',
  'barking',
  'coprophagia',
  'compulsion',
  'house_soiling',
  'rep_materials',
  'hyperactive',
  'destructive',
  'escape',
  'mounting'
)
outcomes <- sort(outcomes)
```

## Fischer Exact

We check for correlation between the predictor (attending puppy training) and
each outcome (the presence of a specific behavior problem). Here we perform the
Fisher Exact test with a Benjamini-Hochberg correction.

```{r}
pred <- 'train_6mo_or_less'

idx <- NULL
p_values <- NULL
odds_ratios <- NULL 
for (outcome in outcomes) {
  tbl <- table(df[, pred], df[, outcome], dnn=c(pred, outcome))
  fisher <- fisher.test(tbl)
  idx <- c(idx, outcome)
  p_values <- c(p_values, fisher$p.value)
  odds_ratios <- c(odds_ratios, fisher$estimate[[1]])
}

# Correct for the possibility of Type I errors.
p_values <- p.adjust(p_values, method='BH')

# Form a result data frame.
df_out <- data.frame(outcome=idx, p_value=p_values,
                     odds_ratio=odds_ratios)

add_sig_columns <- function(df) {
  df$level <- ''
  df[df$p_value <= .05, 'level'] <- '*'
  df[df$p_value <= .01, 'level'] <- '**'
  df[df$p_value <= .001, 'level'] <- '***'
  
  df$dir <- ''
  df[df$odds_ratio < 1, 'dir'] <- '-'
  df[df$odds_ratio > 1, 'dir'] <- '+'
  
  return (df)
}

df_out <- add_sig_columns(df_out)
print(knitr::kable(df_out))
```

So we see that there appears to be an impact from the puppy training, but this
fails to account for other factors that might be at play.

## Binary Logistic Regression

Let's consider what factors may also come in to play:

- Age
- Sex
- Neuter status
- Acquisition of the dog at 12 w.o. or less

We'll perform logistic regression to determine the impact of these factors. To
perform logistic regression we'll need to ensure out data subsets have enough
responses (`n >= 10`) for each possible answer to be included in the model.

```{r}
apply_min_xtab <- function(df, outcome, cutoff=10)
{
  drops <- NULL
  for (col in names(df)) {
    if (col == outcome) next
    if (is.integer(df[,col])) next
    
    xtab <- table(df[,col], df[,outcome])
    if (min(xtab) < cutoff) {
      drops <- c(drops, col)
      break
    }
  }
  
  if (length(drops) > 0) {
    cat('\nDropped from model due to insufficient responses:\n')
    cat(drops)
    cat('\n')
  }
  
  return(df[, !(names(df) %in% drops)])
}
```

Now we perform the logistic regression.

```{r}
pred <- 'train_6mo_or_less'
glm_attribs <- c(
  'age_yrs',
  'male',
  'neutered',
  'acq_12_wo_or_less'
)
for (outcome in outcomes) {
  cat(paste(replicate(80, '-'), collapse=''))
  cat(paste0('\n', outcome, '\n'))
  f <- as.formula(paste0(outcome, '~', '.'))
  
  df_tmp <- df[,c(outcome, pred, glm_attribs)]
  df_tmp <- apply_min_xtab(df_tmp, outcome)
  
  glm_fit <- glm(f, data=df_tmp, family='binomial')
  print(summary(glm_fit))
  print(exp(cbind(OR=coef(glm_fit), suppressMessages(confint(glm_fit)))))
  cat('\nVIF:\n')
  print(car::vif(glm_fit))
  cat('\n')
}
```

# Investigating Training Specifics

At this point we have identified that puppy training has a significant impact on
certain behavior problems. Now we want to investigate the impact of certain
training factors.

## Isolating the Experimental Data Set

First, we isolate the experimental group (those who attended puppy training) in
to their own data set.

```{r}
df_exp <- df %>%
  filter(train_6mo_or_less == TRUE)
summary(df_exp)
```

## Binary Logistic Regression

We want to answer the following questions about the training:

- Did training in the 1-3 month period produce a better outcome than the 4-6
month period?
- Did training technique (reward vs. punishment) have an impact on the
outcome?
- Did the number of sessions have an impact on the outcome?
- Did the choice of restraining device have an impact on the outcome?

We will need to expand the independent variables used for the model to answer
these questions.

```{r}
df <- df %>%
  mutate(reward = ifelse(
    is.na(train_technique), NA, ifelse(
      train_technique == 'reward', TRUE, FALSE)))

devices <- c(
  'buckle_collar',
  'martingale',
  'slip_collar',
  'shock_collar',
  'harness',
  'head_halter',
  'choke_collar',
  'prong_collar'
)
training_params <- c(
  'train_1_3_mo',
  'train_4_mo',
  'train_5_6_mo',
  'reward',
  'train_class_count'
)
glm_attribs <- c(
  glm_attribs,
  devices,
  training_params
)

print(glm_attribs)
```

Now we can build an evaluate our models for the various outcomes.

```{r}
for (outcome in outcomes) {
  cat(paste(replicate(80, '-'), collapse=''))
  cat(paste0('\n', outcome, '\n'))
  f <- as.formula(paste0(outcome, '~', '.'))
  
  df_tmp <- df[,c(outcome, glm_attribs)]
  df_tmp <- apply_min_xtab(df_tmp, outcome)
  
  # If necessary, drop columns due to separability problems.
  if (outcome == 'destructive') {
    df_tmp <- subset(df_tmp, select=-c(shock_collar))
  }
  
  glm_fit <- glm(f, data=df_tmp, family='binomial')
  print(summary(glm_fit))
  print(exp(cbind(OR=coef(glm_fit), suppressMessages(confint(glm_fit)))))
  cat('\nVIF:\n')
  print(car::vif(glm_fit))
  cat('\n')
}
```

# Trends in Dog Age

## Impact of Age on Training Attendance

In our exploratory data analysis we saw a trend that seemed to indicate that
younger dogs were more likely to have attended puppy training than older dogs.
If true, this would seem to suggest that puppy training is becoming more
popular over time. To start, we introduce the question to be answered:

- Were younger dogs more likely to attend more sessions?

### Binomial Logistic Regression

To answer this, we build a regression model where the predictor is the dog's age
and the outcome is whether or not they attended puppy training.

```{r}
pred <- 'age_yrs'
outcome <- 'train_6mo_or_less'
df_tmp <- df[,c(pred, outcome)]
df_tmp <- apply_min_xtab(df_tmp, outcome)
f <- as.formula(paste0(outcome, '~', '.'))
glm_fit <- glm(f, data=df_tmp, family='binomial')
summary(glm_fit)
print(exp(cbind(OR=coef(glm_fit), suppressMessages(confint(glm_fit)))))
```

### Linear Regression

Factors plot poorly, so let's extract the probability of attendance for each age
range and fit it with a linear regression model.

```{r}
p_attend_vec <- NULL
max_age <- max(df$age_yrs)
for (i in 1:max_age) {
  df_tmp <- df %>%
    filter(age_yrs == i) %>%
    select(train_6mo_or_less)
  p <- sum(df_tmp$train_6mo_or_less)/length(df_tmp$train_6mo_or_less)
  p_attend_vec <- c(p_attend_vec, p)
}

df_age <- data.frame(age=c(1:max_age), p_attend=p_attend_vec)
head(df_age)

# Remove the outlier.
outlier <- df_age[15,]
df_age <- df_age[-c(15),]

lm_fit <- lm(p_attend~age, data=df_age)
summary(lm_fit)
print(confint(lm_fit))

old_par <- par(mfrow=c(2,2))
plot(lm_fit)
par(old_par)
```

TODO: observations

### Polynomial Regression

```{r}
plm_fit <- lm(p_attend~poly(age, 3), data=df_age)
summary(plm_fit)
print(confint(plm_fit))

old_par <- par(mfrow=c(2,2))
plot(plm_fit)
par(old_par)
```

The slight increase in explained variance from the polynomial model does not
justify the significant increase in model complexity.

### Visualizing the Trend

Now that we have the age and probability of attendance in a data frame and have
verified a significant fit, let's use it to create out visual.

```{r}
df_age %>%
  ggplot(aes(x=age, y=p_attend)) +
  geom_point(col='cornflowerblue') +
  stat_smooth(method='lm', col='cornflowerblue', fill='coral',
              formula=y~poly(x, 1), alpha=.25) +
  labs(x='Age (years)', y=expression('P'[attendance])) +
  scale_y_continuous(labels=scales::percent) +
  geom_point(x=outlier$age, y=outlier$p_attend, colour='red', shape=4) +
  scale_x_continuous(breaks=seq(1, 20, by=2)) +
  ggtitle('Probability of Early Training Attendance versus Age in Years') +
  theme(plot.title=element_text(hjust = 0.5))
```

## Impact of Age on Problematic Jumping

We saw in our inferential analysis that increased age was correlated with a
decreated probability for jumping up. We want to visualize this trend, so let's
create a data frame with the probability of jumping up for each year of age.

```{r}
p_jump_vec <- NULL
for (i in 1:max_age) {
  df_tmp <- df %>%
    filter(age_yrs == i) %>%
    select(jumping)
  p <- sum(df_tmp$jumping)/length(df_tmp$jumping)
  p_jump_vec <- c(p_jump_vec, p)
}

df_p_jump <- data.frame(age=c(1:max_age), p_jump=p_jump_vec)
head(df_p_jump)
```

Now we create a visualization for this trend.

```{r}
df_p_jump %>%
  ggplot(aes(x=age, y=p_jump)) +
  geom_point(col='cornflowerblue') +
  stat_smooth(method='lm', col='cornflowerblue', fill='coral',
              formula=y~poly(x, 1), alpha=.25) +
  labs(x='Age (years)', y=expression('P'[jumping])) +
  scale_y_continuous(labels=scales::percent) +
  geom_point(x=outlier$age, y=outlier$p_attend, colour='red', shape=4) +
  scale_x_continuous(breaks=seq(1, 20, by=2)) +
  ggtitle('Probability of Problematic Jumping versus Age in Years') +
  theme(plot.title=element_text(hjust = 0.5))
```
